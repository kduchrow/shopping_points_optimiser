# ğŸ›ï¸ Shopping Points Optimiser

Find the best bonus programs and cashback for your online shopping.

**Website:** [shopping-optimiser.de](https://shopping-optimiser.de)

[![CI Pipeline](https://github.com/kduchrow/shopping_points_optimiser/actions/workflows/ci.yml/badge.svg)](https://github.com/kduchrow/shopping_points_optimiser/actions/workflows/ci.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âœ¨ Core Features

- ğŸ›’ **Shop Matching** - Recognizes 1000+ online retailers
- ğŸ’° **Rate Comparison** - Compares bonus programs (Payback, Miles & More, Shoop, TopCashback, etc.)
- ğŸ¤– **Automated Rates** - Scrapers keep rates fresh
- ğŸ‘¥ **Community Driven** - Users propose and vote on rate updates
- ğŸŒ **Browser Extension** - Shopping assistant for Chrome/Edge (v1.0.2)
- â­ **Favorites** - Save your preferred bonus programs
- ğŸ” **User Accounts** - Track proposals and rate history

## ğŸš€ Getting Started

### Installation

```bash
# Clone repository
git clone https://github.com/kduchrow/shopping_points_optimiser.git
cd shopping_points_optimiser

# Create virtual environment & install
python -m venv .venv
.venv\Scripts\activate  # Windows
pip install -r requirements.txt

# Optional: Install worker dependencies (Playwright for Miles&More)
# pip install -r requirements-worker.txt
# playwright install chromium

# Copy environment
cp .env.example .env

# Start services
docker-compose up -d
python app.py
```

Access at: **http://localhost:5000**

### Scraper Worker (optional)

Scrapers run in a separate worker container and report results via the API.
Ensure `SCRAPER_API_TOKEN` is set in your `.env`. The worker uses Redis via `REDIS_URL`.

### Browser Extension

Install from [browser_extension/](browser_extension/README.md) directory (Chrome/Edge).

## ğŸ“– Documentation

### Core Documentation

- **[QUICKSTART](docs/QUICKSTART.md)** - Setup & first steps
- **[DATA MODEL & WORKFLOWS](docs/DATA_MODEL_AND_WORKFLOWS.md)** - Database structure, URL proposals, shop merging

### Technical Guides

- **[Feature Development](docs/FEATURE_DEVELOPMENT_WORKFLOW.md)** - Contributing workflow
- **[Miles & More Implementation](docs/MILES_AND_MORE_SCRAPER_IMPLEMENTATION.md)** - Scraper architecture
- **[Admin System](docs/ADMIN_SYSTEM.md)** - Admin dashboard usage

## ğŸ—ï¸ Architecture

The application uses a **worker-based scraper architecture** to efficiently manage bonus program rate collection:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WORKER-BASED ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  APP CONTAINER (v0.3.1)           WORKER CONTAINER (v0.1.0)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Flask + Gunicorn        â”‚     â”‚  Playwright + RQ         â”‚     â”‚
â”‚  â”‚  (lightweight, no Py)    â”‚     â”‚  (all scrapers)          â”‚     â”‚
â”‚  â”‚                          â”‚     â”‚                          â”‚     â”‚
â”‚  â”‚  REST API Endpoints:     â”‚     â”‚  Scraper Workers:        â”‚     â”‚
â”‚  â€¢ POST /api/scrape-jobs â”‚â”€â”€â”€â”€â†’â”‚  â€¢ Payback (REST API)    â”‚     â”‚
â”‚  â€¢ POST /api/scrape-res. â”‚     â”‚  â€¢ Shoop (REST API)      â”‚     â”‚
â”‚  â”‚  â€¢ [other routes]        â”‚     â”‚  â€¢ TopCashback (Crawl)   â”‚     â”‚
â”‚  â”‚                          â”‚â†â”€â”€â”€â”€â”‚  â€¢ Miles&More (Playwright)â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚           â†“                                 â†‘                        â”‚
â”‚      PostgreSQL                       Redis Queue                   â”‚
â”‚      (ingests results)           (job distribution)                 â”‚
â”‚                                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  KEY FEATURES:                                                       â”‚
â”‚  âœ… Unified Scraper Format: [{name, rates[], source_id}]           â”‚
â”‚  âœ… Batch Processing: 50 shops/request (prevents timeouts)         â”‚
â”‚  âœ… Timeouts: 120s (reasonable for batch operations)               â”‚
â”‚  âœ… Independent Versioning: App & Worker versions separate          â”‚
â”‚  âœ… API-First Design: Workers communicate via REST, not DB access  â”‚
â”‚  âœ… Scalable: Add more workers for parallel scraping               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Services

| Component             | Purpose                                      | Version |
| --------------------- | -------------------------------------------- | ------- |
| **Flask API**         | Web application & REST endpoints             | 0.3.1   |
| **PostgreSQL**        | Primary data store with migrations (Alembic) | 16      |
| **Redis Queue**       | Job distribution & worker communication      | 7       |
| **Scraper Workers**   | Automated bonus program rate collection      | 0.1.0   |
| **Browser Extension** | Client-side shop recognition & rate lookup   | 1.0.2   |

### Scraper Technologies

Each bonus program scraper uses the most efficient approach:

| Scraper         | Technology | Data Source                           | Speed  |
| --------------- | ---------- | ------------------------------------- | ------ |
| **Payback**     | REST API   | Official API                          | âš¡âš¡âš¡ |
| **Shoop**       | REST API   | Official API (categories & merchants) | âš¡âš¡âš¡ |
| **TopCashback** | HTML Crawl | Website category pages                | âš¡âš¡   |
| **Miles&More**  | Playwright | Cloudflare-protected site             | âš¡     |

**Note:** All scrapers run in the worker container. The main app stays lightweight (no Playwright).

## ğŸ‘¨â€ğŸ’» Development

```bash
# Install dev tools
pip install -r requirements-dev.txt

# Pre-commit hooks
pre-commit install

# Run tests
pytest tests/

# Format code
black .
ruff check --fix .
```

## ğŸ“ License

MIT â€“ See [LICENSE](LICENSE) for details.

---

**For questions or feedback:** [shopping-optimiser.de](https://shopping-optimiser.de)

### Environment Variables

Create a `.env` file from `.env.example`:

```bash
# Database Configuration
DATABASE_URL=postgresql+psycopg2://spo:spo@db:5432/spo

# Flask Configuration
SECRET_KEY=your-secret-key-here
DEBUG=False

# Optional
GITHUB_REPO_URL=https://github.com/kduchrow/shopping_points_optimiser
```

### Database

**PostgreSQL** (Production & Development):

- Connection string: `postgresql+psycopg2://spo:spo@db:5432/spo`
- Managed via Alembic migrations
- Auto-migration on Docker container startup

**SQLite** (Legacy, deprecated):

- See [Migration Guide](docs/MIGRATION_SQLITE_TO_POSTGRES.md) for upgrading

### Database Migrations

```bash
# Create new migration
python -m alembic revision --autogenerate -m "Description"

# Apply migrations
python -m alembic upgrade head

# Rollback one migration
python -m alembic downgrade -1

# View migration history
python -m alembic history
```

### Migrate from SQLite to PostgreSQL

```bash
python scripts/migrate_sqlite_to_postgres.py
```

See [detailed migration guide](docs/MIGRATION_SQLITE_TO_POSTGRES.md).

## ğŸ“Š API Endpoints

### Public

- `GET /` - Home page
- `GET /login` - Login page
- `POST /login` - Login
- `GET /register` - Registration

### Admin

- `GET /admin` - Admin panel
- `POST /admin/run_miles_and_more` - Start M&M scraper
- `POST /admin/run_payback` - Start Payback scraper
- `GET /admin/job_status/<id>` - Job status

### Notifications

- `GET /api/notifications` - Get all notifications
- `GET /api/notifications/unread_count` - Unread count
- `POST /api/notifications/<id>/read` - Mark as read
- `POST /api/notifications/read_all` - Mark all as read

### Shop Merges

- `GET /admin/shops/merge_proposals` - List proposals
- `POST /admin/shops/merge_proposal` - Create proposal
- `POST /admin/shops/merge_proposal/<id>/approve` - Approve
- `POST /admin/shops/merge_proposal/<id>/reject` - Reject

### Rate Reviews

- `POST /admin/rate/<id>/comment` - Add comment
- `GET /admin/rate/<id>/comments` - Get comments

## ğŸ§ª Testing

```bash
# Run all tests with coverage
pytest --cov=spo --cov-report=html

# Run specific test file
pytest tests/test_notifications.py

# Run with verbose output
pytest -v

# Demo scripts
python tests/demo_dedup.py
python tests/demo_admin.py
```

### CI/CD Pipeline

GitHub Actions workflow runs on every push/PR:

1. **Lint & Format** - Ruff code analysis
2. **Type Check** - Pyright static analysis
3. **Tests** - pytest with PostgreSQL service
4. **Alembic Check** - Migration validation
5. **Security** - bandit, safety, detect-secrets

See [CI workflow](.github/workflows/ci.yml) for details.

## ğŸ“– Documentation

- [Changelog](docs/CHANGELOG.md) - Version history and release notes
- [Quick Start Guide](docs/QUICKSTART.md) - Detailed setup instructions
- [Admin System Documentation](docs/ADMIN_SYSTEM.md) - Admin features guide
- [SQLite to PostgreSQL Migration](docs/MIGRATION_SQLITE_TO_POSTGRES.md) - Database migration guide
- [Pre-commit Setup](docs/PRE_COMMIT_SETUP.md) - Code quality automation
- [Deployment Guide](docs/DEPLOYMENT.md) - Production deployment

## ğŸ› ï¸ Development

### Code Quality

This project uses automated code quality tools:

- **ruff** - Fast Python linter with auto-fix
- **black** - Code formatter (100 char line-length)
- **isort** - Import sorting
- **pyright** - Type checking
- **prettier** - YAML, JSON, HTML, Markdown formatting
- **yamllint** - YAML validation
- **detect-secrets** - Secret detection

All tools run automatically via pre-commit hooks. See [Pre-commit Setup](docs/PRE_COMMIT_SETUP.md).

### Pre-commit Hooks

```bash
# Install hooks (one-time)
pre-commit install

# Run manually on all files
pre-commit run --all-files

# Run on staged files only
pre-commit run

# Update hooks to latest versions
pre-commit autoupdate
```

### Adding a New Scraper

1. Create scraper in `scrapers/your_scraper.py`
2. Inherit from `BaseScraper`
3. Implement `fetch()` method
4. Use `get_or_create_shop_main()` for deduplication
5. Register in admin routes

### Database Migrations

```bash
# After model changes, create migration
python -m alembic revision --autogenerate -m "Add new field"

# Review generated migration in migrations/versions/
# Apply migration
python -m alembic upgrade head
```

## ğŸ”’ Security

- âœ… Password hashing (werkzeug)
- âœ… CSRF protection
- âœ… Role-based access control
- âœ… Input validation
- âš ï¸ Change `SECRET_KEY` in production!
- âš ï¸ Change default passwords!

## ğŸ“ License

MIT License - See LICENSE file

## ğŸš€ Production CI/CD & Deployment (2026 Update)

### Automated CI/CD Pipeline

- **GitHub Actions**: Automated build, test, and deployment pipeline.
- **Build & Push**: Docker image is built and pushed to GitHub Container Registry (GHCR) on every push to `main` or `ci/deployment`.
- **Deploy Job**: Uses SCP and SSH to deploy to your production server.

### Production Deployment

- **docker-compose.prod.override.yml**: Used for production deployments. Integrates Traefik v3 as a reverse proxy, Gunicorn for WSGI, and all configuration via environment variables.
- **Traefik v3**: Handles HTTPS (Let's Encrypt), domain routing, and secure entrypoints. Configurable via `.env` and GitHub secrets.
- **Gunicorn Workers**: Number of workers is configurable via environment variable (`GUNICORN_WORKERS`).
- **.env Generation**: The deploy workflow creates a `.env` file on the server from GitHub repository variables and secrets, ensuring all sensitive data is never committed.
- **Custom SSH Port**: Deployment supports custom SSH ports via the `SSH_PORT` variable. Ensure your firewall/security group allows access from GitHub Actions runners.

### Deployment File Compliance

- All deployment and override files are fully yamllint and pre-commit compliant (no comments, no deprecated fields).

### Retrying Deployment

- **Manual Dispatch**: Go to GitHub â†’ Actions â†’ CI/CD Shopping Points Optimiser â†’ Run workflow (top right) to manually trigger a deployment.
- **Branch Push**: Pushing to `main` or `ci/deployment` will also trigger the workflow.

See `.github/workflows/deploy.yml` and `docker-compose.prod.override.yml` for implementation details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing`)
3. Install pre-commit hooks (`pre-commit install`)
4. Make changes and commit (`git commit -m 'Add amazing feature'`)
5. Ensure all tests pass (`pytest`)
6. Ensure pre-commit checks pass (`pre-commit run --all-files`)
7. Push to branch (`git push origin feature/amazing`)
8. Open Pull Request

### Code Style

- Python 3.11+ type hints required
- Use modern union syntax: `X | None` instead of `Optional[X]`
- Follow PEP 8 (enforced by ruff/black)
- 100 character line length
- Document public APIs with docstrings

## ğŸ“ Support

- **Documentation**: [docs/](docs/)
- **Issues**: [GitHub Issues](https://github.com/kduchrow/shopping_points_optimiser/issues)
- **Changelog**: [docs/CHANGELOG.md](docs/CHANGELOG.md)
- **Version**: See footer in application UI or [spo/version.py](spo/version.py)

## ğŸ¯ Roadmap

### âœ… Completed

- [x] PostgreSQL database with Alembic migrations
- [x] CI/CD pipeline with GitHub Actions
- [x] Pre-commit hooks for code quality
- [x] Version management and changelog
- [x] Docker-based development and deployment
- [x] Shop deduplication with fuzzy matching
- [x] Merge proposals and community approval workflow
- [x] Admin rescore functionality for shop variants
- [x] Container-only test execution
- [x] Browser Extension with shop recognition (v1.0.2)
- [x] Favorites feature for users
- [x] URL proposal approval workflow
- [x] User authentication & role-based access
- [x] Rate history and deduplication

### ğŸ”„ In Progress

- [ ] Email notifications
- [ ] Advanced analytics dashboard

### ğŸ“‹ Feature Backlog

#### ğŸ¤– Scrapers & Integrations (High Priority)

**Cashback-Plattformen (ğŸ”´ CRITICAL):**

- [x] **Payback Scraper** - Deutsche Cashback-Plattform
- [x] **Miles & More Scraper** - Lufthansa Miles Program
- [x] **Shoop Scraper** - Deutsche Cashback-Plattform
- [x] **TopCashback Scraper** - EuropÃ¤ische Plattform
- [ ] **iGraal Scraper** (M) - FranzÃ¶sische Alternative, deutsche Nutzer
- [ ] **Cashback-Vergleich pro Shop** (L) - Eine Shop-Seite zeigt alle Cashback-Wege
- [ ] **Automatischer Cashback-Rate-Update** (S) - TÃ¤gliche/wÃ¶chentliche Syncs

#### ğŸŸï¸ Coupon-Features (High Priority)

- [ ] **Coupon Up/Downvoting** - Community kann Coupons bewerten
- [ ] Coupon-Ablaufdatum automatisch tracken
- [ ] Automatische Deaktivierung abgelaufener Coupons
- [ ] Coupon-Kategorien (Prozent, Festbetrag, Gratis-Versand)
- [ ] User-Kommentare zu Coupons
- [ ] "Coupon erfolgreich genutzt" Feedback-System

#### ğŸ“Š Daten-QualitÃ¤t & Analytics

**Rate Management:**

- [ ] Rate-History: Verlauf von Bonus-Ã„nderungen Ã¼ber Zeit
- [ ] Benachrichtigungen bei Rate-Ã„nderungen fÃ¼r Favoriten-Shops
- [ ] Trend-Analysen: Welche Shops verbessern/verschlechtern sich
- [ ] Saisonale Analysen (z.B. Black Friday Bonus-Spitzen)
- [ ] Best-Rate-Alert: Benachrichtigung bei besonders guten Rates

**Shop-Verwaltung:**

- [ ] Automatische Shop-URL-Verifizierung
- [ ] Shop-Logo automatisch von Clearbit/Brandfetch holen
- [ ] Duplicate-Shop-Detection verbessern (z.B. amazon.de vs amazon.com)
- [ ] Shop-Kategorien (Fashion, Elektronik, Lebensmittel, etc.)

#### ğŸ¨ User Experience

**Navigation & Filter:**

- [ ] Erweiterte Filter auf Index-Seite (Kategorie, Min-Rate, etc.)
- [ ] Sortierung: Beste Rate, Neueste, Alphabetisch
- [ ] Volltextsuche Ã¼ber Shop-Namen
- [ ] Favoriten-Shops speichern und highlighten
- [ ] KÃ¼rzlich angesehen / Verlauf

**Personalisierung:**

- [ ] Personalisierte Empfehlungen basierend auf genutzten Programmen
- [ ] "Meine Programme" Profile (Nutzer wÃ¤hlt aktive Programme aus)
- [ ] Benachrichtigungen fÃ¼r neue Coupons bei Favoriten
- [ ] Dark Mode

**Export & Sharing:**

- [ ] CSV-Export von Ergebnissen
- [ ] PDF-Report Generator
- [ ] Teilen-Funktion fÃ¼r Shops (Social Share Links)
- [ ] QR-Code fÃ¼r Shop-URLs

#### ğŸŒ Community & Social

- [ ] Shop-Bewertungen (5-Sterne-System) neben Rates
- [ ] User-Reputation-System (Punkte fÃ¼r erfolgreiche Proposals)
- [ ] Badges fÃ¼r aktive Contributors
- [ ] Top-Contributors Leaderboard
- [ ] Newsletter fÃ¼r neue Rates/Coupons (opt-in)
- [ ] User-Profil: PersÃ¶nliche Einsparungen tracking

#### ğŸ“± Mobile & Accessibility

- [ ] Progressive Web App (PWA) Support
- [ ] Browser-Extension (Chrome/Firefox) fÃ¼r schnellen Zugriff
- [ ] Mobile-optimierte UI
- [ ] Barrierefreiheit (WCAG 2.1 AA)

#### ğŸ”§ Technical Improvements

**API & Integration:**

- [ ] GraphQL API zusÃ¤tzlich zu REST
- [ ] Webhook-System fÃ¼r externe Integrationen
- [ ] Rate-Limiting fÃ¼r API
- [ ] API-Dokumentation mit OpenAPI/Swagger

**Performance:**

- [ ] Redis caching layer
- [ ] Database query optimization
- [ ] CDN fÃ¼r statische Assets
- [ ] Lazy-loading fÃ¼r Shop-Listen

**Internationalization:**

- [ ] Multi-Language Support (EN, FR)
- [ ] WÃ¤hrungs-Konvertierung (EUR, CHF, USD)
- [ ] Lokalisierte Shop-Datenbanken

**Testing & Quality:**

- [ ] E2E-Tests mit Playwright
- [ ] Load testing
- [ ] Mutation testing
- [ ] Automated accessibility testing

#### ğŸ” Security & Compliance

- [ ] 2FA (Two-Factor Authentication)
- [ ] GDPR compliance tools (Datenexport, LÃ¶schung)
- [ ] Security audit log
- [ ] Content Security Policy (CSP)

#### ğŸ“ˆ Business Features

- [ ] Affiliate-Link-Integration (z.B. AWIN)
- [ ] Partner-Shops hervorheben
- [ ] Sponsored Coupons
- [ ] Admin-Dashboard mit KPIs (User-Growth, Active Shops, etc.)

---

**PrioritÃ¤ten:**

- ğŸ”´ CRITICAL: Cashback Scrapers (Shoop, TopCashback, iGraal) + Cashback-Vergleich
- ğŸ”´ High: Shop-eigene Bonusprogramme, Coupon-Voting, Rate-History
- ğŸŸ¡ Medium: Favoriten, Filter, Dark Mode, Personalisierte Empfehlungen
- ğŸŸ¢ Low: GraphQL, Badges, Affiliate-Links, Multi-Language

**GeschÃ¤tzte Entwicklungszeit pro Feature:**

- S (Small): 1-3 Tage
- M (Medium): 1-2 Wochen
- L (Large): 3-4 Wochen
- XL (Extra Large): 1-2 Monate

## ğŸ“‹ Versioning

This project follows [Semantic Versioning](https://semver.org/). See [CHANGELOG.md](docs/CHANGELOG.md) for release history.

**Current Version**: See application footer or [spo/version.py](spo/version.py)

---

Made with â¤ï¸ by Shopping Points Optimiser Team
